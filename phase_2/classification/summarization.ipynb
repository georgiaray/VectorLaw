{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc10909c",
   "metadata": {},
   "source": [
    "# LLM to summarize each document for the given question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8258d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d987df",
   "metadata": {},
   "source": [
    "### calling the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "debd9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OpenRouter API key from environment variables\n",
    "OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Please set OPENROUTER_API_KEY in your .env file or environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec056527",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2528c9",
   "metadata": {},
   "source": [
    "## Putting this into a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b4b07",
   "metadata": {},
   "source": [
    "Now that we have seen this work in isolation, let's operationalize it so we can have summaries saved for each of the policies of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d881f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_summarization(doc_prompts):\n",
    "    summaries = []\n",
    "    for idx, prompt in enumerate(doc_prompts):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompts.SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        text_response = response.choices[0].message.content\n",
    "        summaries.append(text_response)\n",
    "        print(f'Assessed question: question_{idx}_prompt')\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d11e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_path = \"../data/scraped_documents/\"\n",
    "all_docs = os.listdir(all_docs_path)\n",
    "all_docs = [doc for doc in all_docs if doc.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3893e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_prompts(doc_text):\n",
    "    question_2_prompt = prompts.summarizing_prompt(doc_text, \"sectoral focus\", prompts.question_2_json_schema)\n",
    "    question_3_prompt = prompts.summarizing_prompt(doc_text, \"subject of intervention\", prompts.question_3_json_schema, prompts.question_3_note)\n",
    "    question_4_prompt = prompts.summarizing_prompt(doc_text, \"market failure\", prompts.question_4_json_schema)\n",
    "    question_5_prompt = prompts.summarizing_prompt(doc_text, \"type of instrument\", prompts.question_5_json_schema)\n",
    "    question_6_prompt = prompts.summarizing_prompt(doc_text, \"metadata and logistical details\", prompts.question_6_json_schema, prompts.question_6_note)\n",
    "    all_prompts = [question_2_prompt, question_3_prompt, question_4_prompt, question_5_prompt, question_6_prompt]\n",
    "\n",
    "    return all_prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a73bea",
   "metadata": {},
   "source": [
    "## Perform summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc73e6",
   "metadata": {},
   "source": [
    "Fix this so it works of unique ids instead of names!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7a0880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping OYQNb4RUEiT0b8Hk.txt: summary already exists for OYQNb4RUEiT0b8Hk.\n",
      "Skipping ldyDWFana7Mtg9DA.txt: summary already exists for ldyDWFana7Mtg9DA.\n",
      "Skipping FfzBsvXzD9fkY2WQ.txt: summary already exists for FfzBsvXzD9fkY2WQ.\n",
      "Skipping Y4A9q1tIiEMnRZWG.txt: summary already exists for Y4A9q1tIiEMnRZWG.\n",
      "Skipping hUDZml18YmFdtiCU.txt: summary already exists for hUDZml18YmFdtiCU.\n",
      "Skipping hYFpxqkLQQDZxafo.txt: summary already exists for hYFpxqkLQQDZxafo.\n",
      "Skipping OP35ITBZ63LXSMj9.txt: summary already exists for OP35ITBZ63LXSMj9.\n",
      "Skipping fbjFEgjb9FLcJ2e6.txt: summary already exists for fbjFEgjb9FLcJ2e6.\n",
      "Skipping K3RMAeovpdI1sbWI.txt: summary already exists for K3RMAeovpdI1sbWI.\n",
      "Skipping aeoKi1dhWO8Tmoyi.txt: summary already exists for aeoKi1dhWO8Tmoyi.\n",
      "working on:  vZaY5DRdtKwXlKQZ.txt\n",
      "Assessed question: question_0_prompt\n",
      "Assessed question: question_1_prompt\n",
      "Assessed question: question_2_prompt\n",
      "Assessed question: question_3_prompt\n",
      "Assessed question: question_4_prompt\n",
      "Skipping dD9SoORj2oSpv9xF.txt: summary already exists for dD9SoORj2oSpv9xF.\n",
      "Skipping YS1EGH0M4083u5y5.txt: summary already exists for YS1EGH0M4083u5y5.\n",
      "Skipping uH9Z494sHCM0SciO.txt: summary already exists for uH9Z494sHCM0SciO.\n",
      "Skipping RwW2YXOyCn3uSq56.txt: summary already exists for RwW2YXOyCn3uSq56.\n",
      "Skipping G6RmGheTDI0EhzXd.txt: summary already exists for G6RmGheTDI0EhzXd.\n",
      "Skipping cD2fv9PBmzMazgmT.txt: summary already exists for cD2fv9PBmzMazgmT.\n",
      "Skipping zxGspsL29aeU1L5v.txt: summary already exists for zxGspsL29aeU1L5v.\n",
      "Skipping asMyDTzVOLLYV94k.txt: summary already exists for asMyDTzVOLLYV94k.\n",
      "Skipping NI2nAhISflAZg6ta.txt: summary already exists for NI2nAhISflAZg6ta.\n",
      "Skipping ekOClvn3B8cMFiLn.txt: summary already exists for ekOClvn3B8cMFiLn.\n",
      "Skipping CC1KXU2vAqsknG1X.txt: summary already exists for CC1KXU2vAqsknG1X.\n",
      "working on:  vswV5GcFju9btwKa.txt\n",
      "Assessed question: question_0_prompt\n",
      "Assessed question: question_1_prompt\n",
      "Assessed question: question_2_prompt\n",
      "Assessed question: question_3_prompt\n",
      "Assessed question: question_4_prompt\n",
      "Skipping s8kuxVUcr121TeqL.txt: summary already exists for s8kuxVUcr121TeqL.\n",
      "Skipping lmScKFSnK8BQ5AOZ.txt: summary already exists for lmScKFSnK8BQ5AOZ.\n",
      "Skipping LSZoWLVPX3VNbyYA.txt: summary already exists for LSZoWLVPX3VNbyYA.\n",
      "Skipping QHLNGcTNy1519Q5Y.txt: summary already exists for QHLNGcTNy1519Q5Y.\n",
      "Skipping nW4piS5hSiCLt9DO.txt: summary already exists for nW4piS5hSiCLt9DO.\n",
      "Skipping awpyR3n5mUyiQX8e.txt: summary already exists for awpyR3n5mUyiQX8e.\n",
      "Skipping wGbrGQkhg7CCZQqu.txt: summary already exists for wGbrGQkhg7CCZQqu.\n",
      "Skipping NRZw2xLsvPzFUWnV.txt: summary already exists for NRZw2xLsvPzFUWnV.\n",
      "Skipping DAum9XBl9BxXQdTP.txt: summary already exists for DAum9XBl9BxXQdTP.\n",
      "Skipping oipp6VI16sVLhSGX.txt: summary already exists for oipp6VI16sVLhSGX.\n",
      "Skipping bNVpsKHG5FpyUF79.txt: summary already exists for bNVpsKHG5FpyUF79.\n",
      "working on:  lBzAmFkhbpQOZMBY.txt\n",
      "Assessed question: question_0_prompt\n",
      "Assessed question: question_1_prompt\n",
      "Assessed question: question_2_prompt\n",
      "Assessed question: question_3_prompt\n",
      "Assessed question: question_4_prompt\n",
      "Skipping 0PKKl2mM9DOjt1r4.txt: summary already exists for 0PKKl2mM9DOjt1r4.\n",
      "Skipping 1NX3gRkASaf3c5mB.txt: summary already exists for 1NX3gRkASaf3c5mB.\n",
      "Skipping d2NNaX5osMjAzQcW.txt: summary already exists for d2NNaX5osMjAzQcW.\n",
      "Skipping iXm1RuiK3wlFEI50.txt: summary already exists for iXm1RuiK3wlFEI50.\n",
      "Skipping 9aurfECiBPdsexq2.txt: summary already exists for 9aurfECiBPdsexq2.\n",
      "Skipping vB8VZgJ3bv4BZxaD.txt: summary already exists for vB8VZgJ3bv4BZxaD.\n",
      "Skipping Ulvgo8aIz4XlTRre.txt: summary already exists for Ulvgo8aIz4XlTRre.\n",
      "Skipping JAYE9ES5gamFNfMr.txt: summary already exists for JAYE9ES5gamFNfMr.\n",
      "Skipping C4tJdLHQMrOXAxJX.txt: summary already exists for C4tJdLHQMrOXAxJX.\n",
      "Skipping NZ0HgoATDLDvdTO1.txt: summary already exists for NZ0HgoATDLDvdTO1.\n",
      "Skipping fzfyQSyoGZV7zyU7.txt: summary already exists for fzfyQSyoGZV7zyU7.\n",
      "Skipping 0617QI04qmv1n2Qn.txt: summary already exists for 0617QI04qmv1n2Qn.\n",
      "Skipping vwQ69gMPHjHuWBky.txt: summary already exists for vwQ69gMPHjHuWBky.\n",
      "Skipping Oqt5iFYxXHEgLYIb.txt: summary already exists for Oqt5iFYxXHEgLYIb.\n",
      "Skipping OA8Deoe89aQHIe5G.txt: summary already exists for OA8Deoe89aQHIe5G.\n",
      "Skipping cjKMpy7ifdtgiu1B.txt: summary already exists for cjKMpy7ifdtgiu1B.\n",
      "Skipping GQ7BHgpfBUyYnAjp.txt: summary already exists for GQ7BHgpfBUyYnAjp.\n",
      "Skipping Gvrfapf3xwftHMMZ.txt: summary already exists for Gvrfapf3xwftHMMZ.\n",
      "Skipping R13QHtkK9rfU3Dqo.txt: summary already exists for R13QHtkK9rfU3Dqo.\n",
      "working on:  ByPueS8fHhoOogr3.txt\n",
      "Assessed question: question_0_prompt\n",
      "Assessed question: question_1_prompt\n",
      "Assessed question: question_2_prompt\n",
      "Assessed question: question_3_prompt\n",
      "Assessed question: question_4_prompt\n",
      "Skipping vDopz6hZTpPeboRn.txt: summary already exists for vDopz6hZTpPeboRn.\n",
      "Skipping K2Ze4u4GiJDQwnV4.txt: summary already exists for K2Ze4u4GiJDQwnV4.\n",
      "Skipping WdPtu3jLrOBm1pHc.txt: summary already exists for WdPtu3jLrOBm1pHc.\n",
      "Skipping TC4JC5YzEYAU9LU8.txt: summary already exists for TC4JC5YzEYAU9LU8.\n",
      "Skipping Ei6uJoE9W1xLKVWu.txt: summary already exists for Ei6uJoE9W1xLKVWu.\n",
      "Skipping aTyiAygVxXVTmKuZ.txt: summary already exists for aTyiAygVxXVTmKuZ.\n",
      "Skipping Zh5iQXVczyUwinGR.txt: summary already exists for Zh5iQXVczyUwinGR.\n",
      "Skipping zBUKOyJgSesgjXyA.txt: summary already exists for zBUKOyJgSesgjXyA.\n",
      "Skipping ORPJeLs4MrFrhZRt.txt: summary already exists for ORPJeLs4MrFrhZRt.\n",
      "Skipping nhEn84cliYvbbOyQ.txt: summary already exists for nhEn84cliYvbbOyQ.\n",
      "working on:  pSqNPdIP2Gr5HetI.txt\n",
      "Assessed question: question_0_prompt\n",
      "Assessed question: question_1_prompt\n",
      "Assessed question: question_2_prompt\n",
      "Assessed question: question_3_prompt\n",
      "Assessed question: question_4_prompt\n",
      "working on:  3AHfTSNelYuhb8eq.txt\n",
      "Truncating base document from 86570 tokens to 38037 tokens.\n",
      "Assessed question: question_0_prompt\n",
      "Assessed question: question_1_prompt\n",
      "Assessed question: question_2_prompt\n",
      "Assessed question: question_3_prompt\n",
      "Assessed question: question_4_prompt\n",
      "Skipping J5swR3IL8UjIABPi.txt: summary already exists for J5swR3IL8UjIABPi.\n",
      "Skipping vAu7yXhg2dOCsQzf.txt: summary already exists for vAu7yXhg2dOCsQzf.\n",
      "Skipping 0LXiNB86ogEwokMr.txt: summary already exists for 0LXiNB86ogEwokMr.\n",
      "Skipping tnaWzSNVgSLTv7XX.txt: summary already exists for tnaWzSNVgSLTv7XX.\n",
      "Skipping vRcited8nUQN31tc.txt: summary already exists for vRcited8nUQN31tc.\n",
      "Skipping QQs3VEGNhdC9YNTb.txt: summary already exists for QQs3VEGNhdC9YNTb.\n",
      "Skipping I8kbRgdLIwbUzZA5.txt: summary already exists for I8kbRgdLIwbUzZA5.\n",
      "Skipping yMcBZVKtNMXz8CAp.txt: summary already exists for yMcBZVKtNMXz8CAp.\n",
      "Skipping aeOGI3ntFMiJPu6i.txt: summary already exists for aeOGI3ntFMiJPu6i.\n",
      "Skipping kXMf5N5fzgvpygh6.txt: summary already exists for kXMf5N5fzgvpygh6.\n",
      "Skipping 3Oahxp3pihpVDQVt.txt: summary already exists for 3Oahxp3pihpVDQVt.\n",
      "Skipping 0bi9qnKBpE62Y3EJ.txt: summary already exists for 0bi9qnKBpE62Y3EJ.\n",
      "Skipping JReryi3dFI1Ou3XO.txt: summary already exists for JReryi3dFI1Ou3XO.\n",
      "Skipping gSPJd9kbjkW9mtL1.txt: summary already exists for gSPJd9kbjkW9mtL1.\n",
      "Skipping IYZ5hi7VuVPGC1Li.txt: summary already exists for IYZ5hi7VuVPGC1Li.\n",
      "Skipping xPHP1quEvFfUh6je.txt: summary already exists for xPHP1quEvFfUh6je.\n",
      "Skipping lC5SRe4SP2pMNuCm.txt: summary already exists for lC5SRe4SP2pMNuCm.\n",
      "Skipping 4F1rb0vmsfkNshQg.txt: summary already exists for 4F1rb0vmsfkNshQg.\n",
      "Skipping 1clGKa1PXAUtLydt.txt: summary already exists for 1clGKa1PXAUtLydt.\n",
      "Skipping Cq8KBARs7aXaVrtK.txt: summary already exists for Cq8KBARs7aXaVrtK.\n",
      "Skipping eRFNvUxTiyJ5mgky.txt: summary already exists for eRFNvUxTiyJ5mgky.\n",
      "Skipping tY4DZOpY0M924Ho4.txt: summary already exists for tY4DZOpY0M924Ho4.\n",
      "working on:  QP801tQAkbwIYFmd.txt\n",
      "Assessed question: question_0_prompt\n",
      "Assessed question: question_1_prompt\n",
      "Assessed question: question_2_prompt\n",
      "Assessed question: question_3_prompt\n",
      "Assessed question: question_4_prompt\n",
      "Skipping vhTFgiS2Ez6okLo6.txt: summary already exists for vhTFgiS2Ez6okLo6.\n",
      "Skipping EkqrNJW7QqSowArI.txt: summary already exists for EkqrNJW7QqSowArI.\n",
      "Skipping X2MKXD8rqn4UJR2S.txt: summary already exists for X2MKXD8rqn4UJR2S.\n",
      "working on:  ee1rsKKIff53QNrE.txt\n",
      "Assessed question: question_0_prompt\n",
      "Assessed question: question_1_prompt\n",
      "Assessed question: question_2_prompt\n",
      "Assessed question: question_3_prompt\n",
      "Assessed question: question_4_prompt\n",
      "Skipping 8Ui54HFehYuBl7ur.txt: summary already exists for 8Ui54HFehYuBl7ur.\n",
      "Skipping K96A0SZrnego6D4a.txt: summary already exists for K96A0SZrnego6D4a.\n",
      "Skipping aJDhGg42rzSRGJvk.txt: summary already exists for aJDhGg42rzSRGJvk.\n",
      "Skipping hP5QkdgsgWb4kZgX.txt: summary already exists for hP5QkdgsgWb4kZgX.\n",
      "Skipping hTLyIMDZMl94b529.txt: summary already exists for hTLyIMDZMl94b529.\n",
      "Skipping XkFKwsVPiBNcSR1B.txt: summary already exists for XkFKwsVPiBNcSR1B.\n",
      "Skipping DueoEkCNSrdjAkzn.txt: summary already exists for DueoEkCNSrdjAkzn.\n",
      "Skipping pKXlmlK8YD7dUfSx.txt: summary already exists for pKXlmlK8YD7dUfSx.\n",
      "Skipping NgdojRq6BkGEpJrS.txt: summary already exists for NgdojRq6BkGEpJrS.\n",
      "Skipping HntYeSkM5RJIhfdb.txt: summary already exists for HntYeSkM5RJIhfdb.\n",
      "Skipping 0N7wzqLvYrInYD4C.txt: summary already exists for 0N7wzqLvYrInYD4C.\n",
      "Skipping uDwI1zGyG5wm2Ij3.txt: summary already exists for uDwI1zGyG5wm2Ij3.\n",
      "Skipping 5LbcbYLQjNkpiCvQ.txt: summary already exists for 5LbcbYLQjNkpiCvQ.\n",
      "Skipping ldhdYC07FZPiAh33.txt: summary already exists for ldhdYC07FZPiAh33.\n",
      "Skipping rHL8KZJcehNVNrBd.txt: summary already exists for rHL8KZJcehNVNrBd.\n",
      "Skipping 2qn97s0DnXYe2qH8.txt: summary already exists for 2qn97s0DnXYe2qH8.\n",
      "Skipping lf0vaU6VpW26FhQV.txt: summary already exists for lf0vaU6VpW26FhQV.\n",
      "Skipping nUz7pimFcfuW0mEp.txt: summary already exists for nUz7pimFcfuW0mEp.\n",
      "Skipping epT5n3eO1Zykzsd7.txt: summary already exists for epT5n3eO1Zykzsd7.\n",
      "Skipping axxzOXaUnYi9W4yB.txt: summary already exists for axxzOXaUnYi9W4yB.\n",
      "Skipping cbRMF5EZrlSlsqq1.txt: summary already exists for cbRMF5EZrlSlsqq1.\n",
      "Skipping w0J7Xs9DoY23oeCt.txt: summary already exists for w0J7Xs9DoY23oeCt.\n",
      "Skipping 5c1S2JtfVqit1gYP.txt: summary already exists for 5c1S2JtfVqit1gYP.\n",
      "Skipping ZOuGV5cjoyVx6sU3.txt: summary already exists for ZOuGV5cjoyVx6sU3.\n",
      "Skipping pApiwZWsnRg6RjbW.txt: summary already exists for pApiwZWsnRg6RjbW.\n",
      "Skipping AQZ39lgN6Bw0OC7r.txt: summary already exists for AQZ39lgN6Bw0OC7r.\n",
      "Skipping 3zDkQRaHMFJvpzUc.txt: summary already exists for 3zDkQRaHMFJvpzUc.\n",
      "Skipping qiVrOYP8RnSaf0Cs.txt: summary already exists for qiVrOYP8RnSaf0Cs.\n",
      "Skipping Dq7tj4kSnARqzlg5.txt: summary already exists for Dq7tj4kSnARqzlg5.\n",
      "Skipping r8upoEKQXtp0YcuD.txt: summary already exists for r8upoEKQXtp0YcuD.\n",
      "Skipping 3yfCv6EhX8efYU8n.txt: summary already exists for 3yfCv6EhX8efYU8n.\n",
      "Skipping OzXmCrgCYiGc1Nk2.txt: summary already exists for OzXmCrgCYiGc1Nk2.\n",
      "working on:  aJoevTRW70STZSq9.txt\n",
      "Assessed question: question_0_prompt\n",
      "Assessed question: question_1_prompt\n",
      "Assessed question: question_2_prompt\n",
      "Assessed question: question_3_prompt\n",
      "Assessed question: question_4_prompt\n",
      "Skipping FFJbyT8KajMmpbwR.txt: summary already exists for FFJbyT8KajMmpbwR.\n",
      "Skipping gptcFi02bUnkbIi3.txt: summary already exists for gptcFi02bUnkbIi3.\n",
      "Skipping vkKzQBjXpTD7UQaP.txt: summary already exists for vkKzQBjXpTD7UQaP.\n",
      "Skipping jZE0iPovmdba5iYu.txt: summary already exists for jZE0iPovmdba5iYu.\n",
      "Skipping zm6ny9KbxXZrYyEM.txt: summary already exists for zm6ny9KbxXZrYyEM.\n",
      "Skipping RWSEQZIqSXiMVrOm.txt: summary already exists for RWSEQZIqSXiMVrOm.\n",
      "Skipping rxkDG0Ai5LbWIkAw.txt: summary already exists for rxkDG0Ai5LbWIkAw.\n",
      "Skipping g2dTE7dek1zuS90I.txt: summary already exists for g2dTE7dek1zuS90I.\n",
      "Skipping PeksDfOg2aZSj8pe.txt: summary already exists for PeksDfOg2aZSj8pe.\n",
      "Skipping MyGTS5jiN6YdasgA.txt: summary already exists for MyGTS5jiN6YdasgA.\n",
      "Skipping d9onOOjfbIELYnc6.txt: summary already exists for d9onOOjfbIELYnc6.\n",
      "Skipping LYUYLMlHz4I8ShSE.txt: summary already exists for LYUYLMlHz4I8ShSE.\n",
      "Skipping 2XUgWVRI3YcBdecj.txt: summary already exists for 2XUgWVRI3YcBdecj.\n",
      "Skipping Hjlnxxej10vaDRFs.txt: summary already exists for Hjlnxxej10vaDRFs.\n",
      "Skipping VIe6wuvINvKrLOwS.txt: summary already exists for VIe6wuvINvKrLOwS.\n",
      "Skipping Invpciu2G0bKmLBR.txt: summary already exists for Invpciu2G0bKmLBR.\n",
      "Skipping cU3N5G1KqrceLGLz.txt: summary already exists for cU3N5G1KqrceLGLz.\n",
      "Skipping uV7xhH8S1B0m0z0d.txt: summary already exists for uV7xhH8S1B0m0z0d.\n",
      "Skipping Di8ayhFMxRTWRYEK.txt: summary already exists for Di8ayhFMxRTWRYEK.\n",
      "Skipping WdhJDTZxXG7sDvCr.txt: summary already exists for WdhJDTZxXG7sDvCr.\n",
      "Skipping nKZmK8c7l1ZL1EfJ.txt: summary already exists for nKZmK8c7l1ZL1EfJ.\n",
      "Skipping 1r3BJDFsjbXU0R3K.txt: summary already exists for 1r3BJDFsjbXU0R3K.\n",
      "Skipping ZAXGO9MsGh015XQD.txt: summary already exists for ZAXGO9MsGh015XQD.\n",
      "Skipping 3D79OgkYjzMU4hqM.txt: summary already exists for 3D79OgkYjzMU4hqM.\n",
      "Skipping 9dY0wMyM1zlVcjSy.txt: summary already exists for 9dY0wMyM1zlVcjSy.\n",
      "working on:  h8BSZsAJj6qzTP6q.txt\n",
      "Assessed question: question_0_prompt\n",
      "Assessed question: question_1_prompt\n",
      "Assessed question: question_2_prompt\n",
      "Assessed question: question_3_prompt\n",
      "Assessed question: question_4_prompt\n",
      "Skipping 0uB7sxk9PSmykFPu.txt: summary already exists for 0uB7sxk9PSmykFPu.\n",
      "Skipping kjNhFstaB37P6TWQ.txt: summary already exists for kjNhFstaB37P6TWQ.\n",
      "Skipping 5yexWjdtsBHm7RRA.txt: summary already exists for 5yexWjdtsBHm7RRA.\n",
      "Skipping I5KJZ47MUEKMk6d2.txt: summary already exists for I5KJZ47MUEKMk6d2.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "summaries_base_path = \"../data/summaries/\"\n",
    "MAX_TOKENS = 128000\n",
    "SAFETY_MARGIN = 1024  # Reserve tokens for prompt/system messages, etc.\n",
    "\n",
    "def prompt_token_count(prompt, encoding=\"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Utility to count tokens in a single prompt.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(encoding)\n",
    "    except KeyError:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(prompt))\n",
    "\n",
    "def available_tokens_for_doc(doc_prompts, max_tokens=MAX_TOKENS, safety_margin=SAFETY_MARGIN, encoding=\"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Determines the maximum tokens that can be used for doc_text\n",
    "    to ensure that the prompt plus system messages fit in context.\n",
    "    \"\"\"\n",
    "    # Estimate length of the largest prompt (prompt is built from doc_text)\n",
    "    prompt_overhead = max([prompt_token_count(p, encoding) for p in doc_prompts]) if doc_prompts else 0\n",
    "    # Just to be sure, allow a safety margin on top of the max_tokens\n",
    "    return max_tokens - prompt_overhead - safety_margin\n",
    "\n",
    "def truncate_text_to_token_limit(text, max_tokens, encoding=\"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Truncate text so its tokenized length is <= max_tokens.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(encoding)\n",
    "    except KeyError:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = enc.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        print(f\"Truncating base document from {len(tokens)} tokens to {max_tokens} tokens.\")\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = enc.decode(tokens)\n",
    "    return text\n",
    "\n",
    "for doc in all_docs:\n",
    "    doc_name_no_ext = os.path.splitext(doc)[0]\n",
    "    doc_summary_folder = os.path.join(summaries_base_path, doc_name_no_ext)\n",
    "    if os.path.exists(doc_summary_folder):\n",
    "        print(f\"Skipping {doc}: summary already exists for {doc_name_no_ext}.\")\n",
    "        continue\n",
    "\n",
    "    print(\"working on: \", doc)\n",
    "    doc_text = open(os.path.join(all_docs_path, doc), \"r\").read()\n",
    "\n",
    "    # Step 1: Estimate what doc_prompts would look like for a chunk of doc_text\n",
    "    # Temporarily build prompts with the full doc_text\n",
    "    temp_doc_prompts = get_all_prompts(doc_text)\n",
    "    # Calculate maximum allowed tokens in doc_text (subtracting prompt overhead and margin)\n",
    "    max_allowed_tokens = available_tokens_for_doc(temp_doc_prompts, max_tokens=MAX_TOKENS, safety_margin=SAFETY_MARGIN)\n",
    "\n",
    "    # Step 2: Actually truncate doc_text to fit\n",
    "    doc_text = truncate_text_to_token_limit(doc_text, max_allowed_tokens)\n",
    "\n",
    "    # Step 3: Build prompts again (using truncated doc_text)\n",
    "    doc_prompts = get_all_prompts(doc_text)\n",
    "    doc_summaries = perform_summarization(doc_prompts)\n",
    "\n",
    "    # Create subfolder for document summary\n",
    "    os.makedirs(doc_summary_folder, exist_ok=True)\n",
    "\n",
    "    # Save each summary as a separate file inside the subfolder\n",
    "    for idx, summary in enumerate(doc_summaries, start=2):\n",
    "        summary_filename = f\"question_{idx}_summary.txt\"\n",
    "        summary_path = os.path.join(doc_summary_folder, summary_filename)\n",
    "        with open(summary_path, \"w\") as f:\n",
    "            f.write(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "law-comparisons-9i-3jzyC-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
