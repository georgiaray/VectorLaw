{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96a6a9c",
   "metadata": {},
   "source": [
    "# Embed and Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f11cac",
   "metadata": {},
   "source": [
    "Because I am going to be taking a RAG approach, this notebook will be a helper notebook to embed and chunk the data scraped from the internet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483bd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb0f5c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be37730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/georgiaray_ic/Documents/coding/law_comparisons/phase_2/data/scraped_documents\n",
      "Vector store directory: /Users/georgiaray_ic/Documents/coding/law_comparisons/phase_2/data/vector_store\n"
     ]
    }
   ],
   "source": [
    "# Resolve key paths relative to this notebook\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "DATA_DIR = (NOTEBOOK_DIR / \"../data/scraped_documents\").resolve()\n",
    "VECTOR_STORE_DIR = (NOTEBOOK_DIR / \"../data/vector_store\").resolve()\n",
    "VECTOR_STORE_PATH = VECTOR_STORE_DIR / \"vector_store.pkl\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Vector store directory: {VECTOR_STORE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc05eaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenRouter client configured.\n"
     ]
    }
   ],
   "source": [
    "# Load credentials and instantiate the OpenRouter client\n",
    "load_dotenv()\n",
    "\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Please set OPENROUTER_API_KEY before running embeddings.\")\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "print(\"OpenRouter client configured.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a1006",
   "metadata": {},
   "source": [
    "## Chunking Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875eaf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 157 documents from /Users/georgiaray_ic/Documents/coding/law_comparisons/phase_2/data/scraped_documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def chunk_text(text: str, chunk_size: int = 200, chunk_overlap: int = 75) -> List[str]:\n",
    "    \"\"\"Split a document into overlapping word chunks of fixed size.\"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    chunks: List[str] = []\n",
    "    start = 0\n",
    "    total_words = len(words)\n",
    "\n",
    "    while start < total_words:\n",
    "        end = min(total_words, start + chunk_size)\n",
    "        chunk = \" \".join(words[start:end]).strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        if end >= total_words:\n",
    "            break\n",
    "        # Decide new start: handle overlap\n",
    "        start += max(1, chunk_size - chunk_overlap)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def load_documents(data_dir: Path) -> Dict[str, str]:\n",
    "    \"\"\"Read every .txt file in the input directory.\"\"\"\n",
    "    documents: Dict[str, str] = {}\n",
    "    for path in sorted(data_dir.glob(\"*.txt\")):\n",
    "        documents[path.stem] = path.read_text(encoding=\"utf-8\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "documents = load_documents(DATA_DIR)\n",
    "print(f\"Loaded {len(documents)} documents from {DATA_DIR}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c709ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import re\n",
    "\n",
    "def trim_non_content(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Attempts to trim repeated navigation/boilerplate 'chrome' text\n",
    "    like headers, navbars, footers, menus, and non-content at the head and tail of the doc.\n",
    "    Now also attempts to aggressively remove 'media contact' and long government footer sections.\n",
    "    Not guaranteed to catch everything, but tries to remove common patterns.\n",
    "    \"\"\"\n",
    "    # Common boilerplate phrases likely indicating start/end of content\n",
    "    head_patterns = [\n",
    "        r\"^(?:.*Canada\\.ca.*\\n){1,5}\",  # Canada.ca spam header\n",
    "        r\"^Skip to main content[\\n\\r]+\", \n",
    "        r\"^Skip to [^\\n]+\\n\", \n",
    "        r\"^Language selection\\n\", \n",
    "        r\"^(?:Français|fr|Gouvernement du Canada)[\\n/ ]+\",\n",
    "        r\"^Search[^\\n]*\\n\", \n",
    "        r\"^Menu\\n\", \n",
    "        r\"^Main\\n\", \n",
    "        r\"^[\\w \\-/]+\\nJobs and the workplace\\n\",  # menu bar spam\n",
    "        r\"^(?:[\\w ,/&-]+\\n){3,10}You are here:[^\\n]*\\n\",  # Common 'menu' preamble\n",
    "        r\"^From:[^\\n]+\\n News release[\\n]*\",  # News release preamble\n",
    "    ]\n",
    "\n",
    "    # Extended tail patterns to catch press/media contact and keyword megamenus\n",
    "    tail_patterns = [\n",
    "        r\"\\nReport a problem or mistake on this page.*$\", \n",
    "        r\"\\nDate modified:[^\\n]*$\", \n",
    "        r\"\\n(?:Footer|End of Document|Contact us)[^\\n]*$\", \n",
    "        r\"\\nThis page was last updated.*$\",\n",
    "        # Pattern for typical \"For media:\" contact/info blocks\n",
    "        r\"\\nFor media:[\\s\\S]+?(?=\\n\\S)\",  # Stop at next headline, non-indented line\n",
    "        # Pattern for \"Media Relations\" sections to end of text or next major block\n",
    "        r\"\\nMedia Relations[\\s\\S]+?(?=\\n\\S|\\Z)\",\n",
    "        # Any block of repeated contact/institution/string-ending info separated by newlines at the end\n",
    "        r\"(?:\\n[\\w \\-.,/:\\(\\)@]+){6,}[\\s\\n]*$\",  # If 6+ consecutive lines of mostly names, contacts, orgs, likely a tail\n",
    "        # Remove \"Search for related information by keyword: ...\" (plus likely following menu/keyword lines)\n",
    "        r\"\\nSearch for related information by keyword:[\\s\\S]+?(?=\\n\\S|\\Z)\",\n",
    "        # Remove \"Page details\", \"About this site\", \"Government of Canada\", mega-menu block at tail\n",
    "        r\"\\nPage details[\\s\\S]+?(?=\\n\\S|\\Z)\",\n",
    "        r\"\\nAbout this site[\\s\\S]+?(?=\\n\\S|\\Z)\",\n",
    "        r\"\\nGovernment of Canada[\\s\\S]+?(?=\\n\\S|\\Z)\",\n",
    "        r\"\\nAll contacts[\\s\\S]+?(?=\\n\\S|\\Z)\",\n",
    "    ]\n",
    "\n",
    "    cleaned = text.strip()\n",
    "\n",
    "    # Heuristically trim head\n",
    "    for pat in head_patterns:\n",
    "        cleaned_new = re.sub(pat, '', cleaned, flags=re.IGNORECASE|re.MULTILINE)\n",
    "        if len(cleaned_new) < len(cleaned) - 8:  # Actually shortened content?\n",
    "            cleaned = cleaned_new\n",
    "            break\n",
    "\n",
    "    # Heuristically trim tail\n",
    "    trimmed = False\n",
    "    for pat in tail_patterns:\n",
    "        cleaned_new = re.sub(pat, '', cleaned, flags=re.IGNORECASE|re.MULTILINE)\n",
    "        if len(cleaned_new) < len(cleaned) - 8:\n",
    "            cleaned = cleaned_new\n",
    "            trimmed = True  # Remove multiple from tail\n",
    "    # Try tail removal twice (to catch two-stage footers)\n",
    "    if trimmed:\n",
    "        for pat in tail_patterns:\n",
    "            cleaned_new = re.sub(pat, '', cleaned, flags=re.IGNORECASE|re.MULTILINE)\n",
    "            if len(cleaned_new) < len(cleaned) - 8:\n",
    "                cleaned = cleaned_new\n",
    "\n",
    "    # Secondary: For repeated menu/footer junk, try to cut on keyword\n",
    "    NON_CONTENT_KEYWORDS = [\n",
    "        \"You are here:\",\n",
    "        \"Main Menu\",\n",
    "        \"Search Canada.ca\",\n",
    "        \"Back to top\",\n",
    "        \"Date modified:\", \n",
    "        \"Report a problem or mistake on this page\",\n",
    "        \"Contact us\",\n",
    "        \"Page details\",\n",
    "        \"About this site\",\n",
    "        \"Government of Canada\",\n",
    "        \"All contacts\",\n",
    "        \"Departments and agencies\",\n",
    "        \"Benefits\",\n",
    "        \"Social media\",\n",
    "        \"Privacy\",\n",
    "        \"Terms and conditions\",\n",
    "        \"Mobile applications\",\n",
    "        \"Themes and topics\",\n",
    "        \"Follow us on\",\n",
    "    ]\n",
    "    # Remove lines at top or bottom containing only these keywords\n",
    "    lines = cleaned.splitlines()\n",
    "    # Remove leading non-content lines\n",
    "    while lines and any(k.lower() in lines[0].lower() for k in NON_CONTENT_KEYWORDS):\n",
    "        lines = lines[1:]\n",
    "    # Remove trailing non-content lines\n",
    "    while lines and any(k.lower() in lines[-1].lower() for k in NON_CONTENT_KEYWORDS):\n",
    "        lines = lines[:-1]\n",
    "\n",
    "    # Remove trailing contact blocks that start with a name/email/phone/office-style sequence\n",
    "    # Heuristic: 4+ lines at end with email/phone/org/office or a lot of short lines\n",
    "    tail_lines = lines[-12:]\n",
    "    for i in range(len(tail_lines)):\n",
    "        slice_ = tail_lines[i:]\n",
    "        # Must be 4+ trailing \"contact-ish\" lines\n",
    "        if (len(slice_) >= 4 and\n",
    "            sum(bool(re.search(r\"\\b(?:@|[0-9]{3}-[0-9]{3}-[0-9]{4}|\\([0-9]{3}\\)|Director|Media|Office|Relations|Ottawa|Canada|Minister)\", l, re.IGNORECASE)) for l in slice_) >= 2):\n",
    "            lines = lines[:-len(slice_)]\n",
    "            break\n",
    "\n",
    "    return \"\\n\".join(lines).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bedccbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    documents[doc] = trim_non_content(documents[doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc60fc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 3504 chunks across all documents.\n"
     ]
    }
   ],
   "source": [
    "chunk_records: List[Dict[str, Any]] = []\n",
    "for doc_name, text in documents.items():\n",
    "    chunks = chunk_text(text)\n",
    "    if not chunks:\n",
    "        continue\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk_records.append(\n",
    "            {\n",
    "                \"doc_name\": doc_name,\n",
    "                \"chunk_index\": idx,\n",
    "                \"text\": chunk,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(f\"Prepared {len(chunk_records)} chunks across all documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922a03c",
   "metadata": {},
   "source": [
    "## Generate and Persist Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "281ee453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 3504 / 3504 chunks\n",
      "Persisted vector store to /Users/georgiaray_ic/Documents/coding/law_comparisons/phase_2/data/vector_store/vector_store.pkl\n"
     ]
    }
   ],
   "source": [
    "def embed_chunk_batch(batch: List[Dict[str, Any]]) -> List[List[float]]:\n",
    "    inputs = [item[\"text\"] for item in batch]\n",
    "    response = client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=inputs\n",
    "    )\n",
    "    # OpenAI returns embeddings in the same order as inputs\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "if chunk_records:\n",
    "    VECTOR_STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    enriched_records: List[Dict[str, Any]] = []\n",
    "    batch_size = 64\n",
    "    for start in range(0, len(chunk_records), batch_size):\n",
    "        batch = chunk_records[start:start + batch_size]\n",
    "        embeddings = embed_chunk_batch(batch)\n",
    "        for record, embedding in zip(batch, embeddings):\n",
    "            enriched = {**record, \"embedding\": embedding}\n",
    "            enriched_records.append(enriched)\n",
    "        print(f\"Embedded {len(enriched_records)} / {len(chunk_records)} chunks\", end=\"\\r\")\n",
    "\n",
    "    # Organize per-document and persist\n",
    "    vector_store: Dict[str, Dict[str, Any]] = {}\n",
    "    for record in enriched_records:\n",
    "        doc_entry = vector_store.setdefault(\n",
    "            record[\"doc_name\"],\n",
    "            {\"embeddings\": [], \"chunks\": []}\n",
    "        )\n",
    "        doc_entry[\"embeddings\"].append(record[\"embedding\"])\n",
    "        doc_entry[\"chunks\"].append(\n",
    "            {\n",
    "                \"chunk_index\": record[\"chunk_index\"],\n",
    "                \"text\": record[\"text\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    for doc_name, doc_entry in vector_store.items():\n",
    "        doc_entry[\"embeddings\"] = np.array(doc_entry[\"embeddings\"], dtype=np.float32)\n",
    "        doc_entry[\"chunks\"] = doc_entry[\"chunks\"]\n",
    "\n",
    "    with open(VECTOR_STORE_PATH, \"wb\") as f:\n",
    "        pickle.dump(vector_store, f)\n",
    "\n",
    "    print(f\"\\nPersisted vector store to {VECTOR_STORE_PATH}\")\n",
    "else:\n",
    "    print(\"No chunks were prepared; vector store was not created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f07c0f5",
   "metadata": {},
   "source": [
    "## Query Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b5ecf",
   "metadata": {},
   "source": [
    "These are also put into a utils file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8388cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(path: Path = VECTOR_STORE_PATH) -> Dict[str, Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Vector store not found at {path}. Run the embedding cell first.\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    # Ensure embeddings are numpy arrays after unpickling\n",
    "    for entry in store.values():\n",
    "        entry[\"embeddings\"] = np.array(entry[\"embeddings\"], dtype=np.float32)\n",
    "    return store\n",
    "\n",
    "\n",
    "def query_document(store: Dict[str, Dict[str, Any]], doc_name: str, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    doc_entry = store.get(doc_name)\n",
    "    if doc_entry is None:\n",
    "        available = \", \".join(sorted(store.keys()))\n",
    "        raise KeyError(f\"Document '{doc_name}' not in vector store. Available names: {available}\")\n",
    "\n",
    "    query_embedding = client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=[query]\n",
    "    ).data[0].embedding\n",
    "\n",
    "    doc_embeddings = doc_entry[\"embeddings\"]\n",
    "    if not len(doc_embeddings):\n",
    "        return []\n",
    "\n",
    "    query_vector = np.array(query_embedding, dtype=np.float32)\n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    doc_norms = np.linalg.norm(doc_embeddings, axis=1)\n",
    "    similarities = (doc_embeddings @ query_vector) / (doc_norms * query_norm + 1e-8)\n",
    "\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for idx in top_indices:\n",
    "        chunk_meta = doc_entry[\"chunks\"][idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"chunk_index\": chunk_meta[\"chunk_index\"],\n",
    "                \"similarity\": float(similarities[idx]),\n",
    "                \"text\": chunk_meta[\"text\"],\n",
    "            }\n",
    "        )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d158954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks in vector store: 3504\n"
     ]
    }
   ],
   "source": [
    "total_chunks = sum(len(entry[\"chunks\"]) for entry in load_vector_store().values())\n",
    "print(f\"Total chunks in vector store: {total_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52a6aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks per document:\n",
      "  0617QI04qmv1n2Qn: 4\n",
      "  0LXiNB86ogEwokMr: 10\n",
      "  0N7wzqLvYrInYD4C: 5\n",
      "  0PKKl2mM9DOjt1r4: 2\n",
      "  0bi9qnKBpE62Y3EJ: 1\n",
      "  0uB7sxk9PSmykFPu: 4\n",
      "  1NX3gRkASaf3c5mB: 33\n",
      "  1clGKa1PXAUtLydt: 18\n",
      "  1r3BJDFsjbXU0R3K: 23\n",
      "  2XUgWVRI3YcBdecj: 1\n",
      "  2qn97s0DnXYe2qH8: 143\n",
      "  3AHfTSNelYuhb8eq: 180\n",
      "  3D79OgkYjzMU4hqM: 4\n",
      "  3Oahxp3pihpVDQVt: 44\n",
      "  3yfCv6EhX8efYU8n: 7\n",
      "  3zDkQRaHMFJvpzUc: 7\n",
      "  4F1rb0vmsfkNshQg: 4\n",
      "  5LbcbYLQjNkpiCvQ: 5\n",
      "  5c1S2JtfVqit1gYP: 19\n",
      "  5yexWjdtsBHm7RRA: 54\n",
      "  8Ui54HFehYuBl7ur: 21\n",
      "  9aurfECiBPdsexq2: 4\n",
      "  9dY0wMyM1zlVcjSy: 6\n",
      "  AQZ39lgN6Bw0OC7r: 5\n",
      "  ByPueS8fHhoOogr3: 17\n",
      "  C4tJdLHQMrOXAxJX: 2\n",
      "  CC1KXU2vAqsknG1X: 4\n",
      "  Cq8KBARs7aXaVrtK: 1\n",
      "  DAum9XBl9BxXQdTP: 53\n",
      "  Di8ayhFMxRTWRYEK: 11\n",
      "  Dq7tj4kSnARqzlg5: 2\n",
      "  DueoEkCNSrdjAkzn: 7\n",
      "  Ei6uJoE9W1xLKVWu: 1\n",
      "  EkqrNJW7QqSowArI: 1\n",
      "  FFJbyT8KajMmpbwR: 5\n",
      "  FfzBsvXzD9fkY2WQ: 2\n",
      "  G6RmGheTDI0EhzXd: 1\n",
      "  GQ7BHgpfBUyYnAjp: 3\n",
      "  Gvrfapf3xwftHMMZ: 1\n",
      "  Hjlnxxej10vaDRFs: 5\n",
      "  HntYeSkM5RJIhfdb: 1\n",
      "  I5KJZ47MUEKMk6d2: 61\n",
      "  I8kbRgdLIwbUzZA5: 5\n",
      "  IYZ5hi7VuVPGC1Li: 3\n",
      "  Invpciu2G0bKmLBR: 602\n",
      "  J5swR3IL8UjIABPi: 10\n",
      "  JAYE9ES5gamFNfMr: 2\n",
      "  JReryi3dFI1Ou3XO: 1\n",
      "  K2Ze4u4GiJDQwnV4: 27\n",
      "  K3RMAeovpdI1sbWI: 1\n",
      "  K96A0SZrnego6D4a: 3\n",
      "  LSZoWLVPX3VNbyYA: 4\n",
      "  LYUYLMlHz4I8ShSE: 12\n",
      "  MyGTS5jiN6YdasgA: 300\n",
      "  NI2nAhISflAZg6ta: 121\n",
      "  NRZw2xLsvPzFUWnV: 11\n",
      "  NZ0HgoATDLDvdTO1: 4\n",
      "  NgdojRq6BkGEpJrS: 31\n",
      "  OA8Deoe89aQHIe5G: 37\n",
      "  OP35ITBZ63LXSMj9: 4\n",
      "  ORPJeLs4MrFrhZRt: 28\n",
      "  OYQNb4RUEiT0b8Hk: 1\n",
      "  Oqt5iFYxXHEgLYIb: 4\n",
      "  OzXmCrgCYiGc1Nk2: 20\n",
      "  PeksDfOg2aZSj8pe: 1\n",
      "  QHLNGcTNy1519Q5Y: 294\n",
      "  QP801tQAkbwIYFmd: 1\n",
      "  QQs3VEGNhdC9YNTb: 8\n",
      "  R13QHtkK9rfU3Dqo: 2\n",
      "  RWSEQZIqSXiMVrOm: 6\n",
      "  RwW2YXOyCn3uSq56: 7\n",
      "  TC4JC5YzEYAU9LU8: 13\n",
      "  Ulvgo8aIz4XlTRre: 3\n",
      "  VIe6wuvINvKrLOwS: 35\n",
      "  WdPtu3jLrOBm1pHc: 1\n",
      "  WdhJDTZxXG7sDvCr: 6\n",
      "  X2MKXD8rqn4UJR2S: 1\n",
      "  XkFKwsVPiBNcSR1B: 125\n",
      "  Y4A9q1tIiEMnRZWG: 70\n",
      "  YS1EGH0M4083u5y5: 1\n",
      "  ZAXGO9MsGh015XQD: 4\n",
      "  ZOuGV5cjoyVx6sU3: 3\n",
      "  Zh5iQXVczyUwinGR: 86\n",
      "  aJDhGg42rzSRGJvk: 34\n",
      "  aJoevTRW70STZSq9: 27\n",
      "  aTyiAygVxXVTmKuZ: 5\n",
      "  aeOGI3ntFMiJPu6i: 2\n",
      "  aeoKi1dhWO8Tmoyi: 2\n",
      "  asMyDTzVOLLYV94k: 1\n",
      "  axxzOXaUnYi9W4yB: 66\n",
      "  bNVpsKHG5FpyUF79: 7\n",
      "  cD2fv9PBmzMazgmT: 1\n",
      "  cU3N5G1KqrceLGLz: 7\n",
      "  cbRMF5EZrlSlsqq1: 5\n",
      "  cjKMpy7ifdtgiu1B: 2\n",
      "  d2NNaX5osMjAzQcW: 3\n",
      "  d9onOOjfbIELYnc6: 6\n",
      "  dD9SoORj2oSpv9xF: 5\n",
      "  eRFNvUxTiyJ5mgky: 19\n",
      "  ee1rsKKIff53QNrE: 1\n",
      "  ekOClvn3B8cMFiLn: 1\n",
      "  epT5n3eO1Zykzsd7: 1\n",
      "  fbjFEgjb9FLcJ2e6: 1\n",
      "  fzfyQSyoGZV7zyU7: 4\n",
      "  g2dTE7dek1zuS90I: 2\n",
      "  gSPJd9kbjkW9mtL1: 1\n",
      "  gptcFi02bUnkbIi3: 3\n",
      "  h8BSZsAJj6qzTP6q: 31\n",
      "  hP5QkdgsgWb4kZgX: 78\n",
      "  hTLyIMDZMl94b529: 3\n",
      "  hUDZml18YmFdtiCU: 3\n",
      "  hYFpxqkLQQDZxafo: 3\n",
      "  iXm1RuiK3wlFEI50: 49\n",
      "  jZE0iPovmdba5iYu: 5\n",
      "  kXMf5N5fzgvpygh6: 10\n",
      "  kjNhFstaB37P6TWQ: 5\n",
      "  lBzAmFkhbpQOZMBY: 5\n",
      "  lC5SRe4SP2pMNuCm: 5\n",
      "  ldhdYC07FZPiAh33: 1\n",
      "  ldyDWFana7Mtg9DA: 3\n",
      "  lf0vaU6VpW26FhQV: 5\n",
      "  lmScKFSnK8BQ5AOZ: 3\n",
      "  nKZmK8c7l1ZL1EfJ: 24\n",
      "  nUz7pimFcfuW0mEp: 8\n",
      "  nW4piS5hSiCLt9DO: 23\n",
      "  nhEn84cliYvbbOyQ: 4\n",
      "  oipp6VI16sVLhSGX: 1\n",
      "  pApiwZWsnRg6RjbW: 8\n",
      "  pKXlmlK8YD7dUfSx: 22\n",
      "  pSqNPdIP2Gr5HetI: 1\n",
      "  qiVrOYP8RnSaf0Cs: 6\n",
      "  r8upoEKQXtp0YcuD: 3\n",
      "  rHL8KZJcehNVNrBd: 5\n",
      "  rxkDG0Ai5LbWIkAw: 1\n",
      "  s8kuxVUcr121TeqL: 3\n",
      "  tY4DZOpY0M924Ho4: 163\n",
      "  tnaWzSNVgSLTv7XX: 4\n",
      "  uDwI1zGyG5wm2Ij3: 5\n",
      "  uH9Z494sHCM0SciO: 6\n",
      "  uV7xhH8S1B0m0z0d: 2\n",
      "  vAu7yXhg2dOCsQzf: 33\n",
      "  vB8VZgJ3bv4BZxaD: 2\n",
      "  vDopz6hZTpPeboRn: 3\n",
      "  vRcited8nUQN31tc: 5\n",
      "  vZaY5DRdtKwXlKQZ: 1\n",
      "  vhTFgiS2Ez6okLo6: 4\n",
      "  vkKzQBjXpTD7UQaP: 8\n",
      "  vswV5GcFju9btwKa: 4\n",
      "  vwQ69gMPHjHuWBky: 1\n",
      "  w0J7Xs9DoY23oeCt: 1\n",
      "  wGbrGQkhg7CCZQqu: 1\n",
      "  xPHP1quEvFfUh6je: 10\n",
      "  yMcBZVKtNMXz8CAp: 3\n",
      "  zBUKOyJgSesgjXyA: 2\n",
      "  zm6ny9KbxXZrYyEM: 3\n",
      "  zxGspsL29aeU1L5v: 39\n"
     ]
    }
   ],
   "source": [
    "vector_store = load_vector_store()\n",
    "chunk_counts = {doc_name: len(entry[\"chunks\"]) for doc_name, entry in vector_store.items()}\n",
    "print(\"Chunks per document:\")\n",
    "for doc_name, count in chunk_counts.items():\n",
    "    print(f\"  {doc_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c527ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document awpyR3n5mUyiQX8e not represented in vector store (name: net-zero challenge)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mapper = pd.read_csv(\"../data/unique_id_to_name.csv\")\n",
    "unique_id_to_name = dict(zip(mapper[\"unique_id\"], mapper[\"name\"]))\n",
    "\n",
    "# Check if all the documents are represented \n",
    "for doc_name in documents.keys():\n",
    "    if doc_name not in vector_store:\n",
    "        readable_name = unique_id_to_name.get(doc_name, \"(unknown name)\")\n",
    "        print(f\"Document {doc_name} not represented in vector store (name: {readable_name})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ff3db",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9225378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top chunks for document: 0617QI04qmv1n2Qn\n",
      "----------------------------------------\n",
      "Chunk #1 (similarity 0.231)\n",
      "to help Canadian onshore oil and gas companies invest in green solutions and infrastructure to continue their progress toward reducing methane emissions while facing the COVID-19 pandemic. The final intake closed on March 31, 2022, and funding eligibility closed by March 31, 2023. Results to date The ERF Onshore Program funded 24 companies, to implement 91 projects across Manitoba, Saskatchewan, Alberta and British Columbia, representing $170M in repayable and partially repayable NRCan funding. These projects are expected to cut an anticipated 4 MT of CO 2 e in their first year after completion. As a part of program administration, the ERF Onshore Program collects data from completed projects and reports on that on an ongoing basis. In addition to the data in the three tables, below, the program has compiled three separate reports. Additionality Report – evaluates the extent to which ERF supported projects generated emissions reductions beyond regulatory requirements Outputs Report – describes program outcomes Socioeconomic & Diversity Benefits Report – describes the socioeconomic benefits associated with ERF funding The Outputs Report and the Socioeconomic & Diversity Benefits Report will be updated in future once all project reports are submitted to the department. The Emissions Reduction Fund funded 91\n",
      "----------------------------------------\n",
      "Chunk #2 (similarity 0.225)\n",
      "below, the program has compiled three separate reports. Additionality Report – evaluates the extent to which ERF supported projects generated emissions reductions beyond regulatory requirements Outputs Report – describes program outcomes Socioeconomic & Diversity Benefits Report – describes the socioeconomic benefits associated with ERF funding The Outputs Report and the Socioeconomic & Diversity Benefits Report will be updated in future once all project reports are submitted to the department. The Emissions Reduction Fund funded 91 projects across 4 provinces under the 3 application periods of the ERF Onshore Program. Intake Number of Projects ERF funding (up to $) Total Project Cost ($) 1 35 $76,049,574 $110,417,712 2 49 $67,630,430 $102,114,812 3 7 $24,924,947 $42,350,715 Total 91 $168,604,951 $254,883,239 Projects by province Province Number of Projects ERF funding (up to $) Total Project Cost ($) Alberta 48 $90,522,392 $138,096,830 Manitoba 3 $15,162,750 $24,361,385 British Columbia 3 $577,653 $770,207 Saskatchewan 37 $62,342,156 $91,654,817 Total 91 $168,604,951 $254,883,239 These amounts are subject to change due to formal amendments and project cancellations, so for the most up-to-date information on funded companies, please refer to the Government of Canada’s *Number of projects may vary annual due to waivers and project cancellations. Reports Emissions Reduction Fund\n",
      "----------------------------------------\n",
      "Chunk #3 (similarity 0.223)\n",
      "ERF funding (up to $) Total Project Cost ($) Alberta 48 $90,522,392 $138,096,830 Manitoba 3 $15,162,750 $24,361,385 British Columbia 3 $577,653 $770,207 Saskatchewan 37 $62,342,156 $91,654,817 Total 91 $168,604,951 $254,883,239 These amounts are subject to change due to formal amendments and project cancellations, so for the most up-to-date information on funded companies, please refer to the Government of Canada’s *Number of projects may vary annual due to waivers and project cancellations. Reports Emissions Reduction Fund Onshore Program: Greenhouse gas accounting methodology and regulatory additionality review Emissions Reduction Fund onshore stream – program outputs report\n"
     ]
    }
   ],
   "source": [
    "if VECTOR_STORE_PATH.exists():\n",
    "    store = load_vector_store()\n",
    "    sample_doc = next(iter(store.keys()))\n",
    "    sample_results = query_document(\n",
    "        store,\n",
    "        doc_name=sample_doc,\n",
    "        query=\"information about market failures\",\n",
    "        top_k=3,\n",
    "    )\n",
    "    print(f\"Top chunks for document: {sample_doc}\")\n",
    "    for result in sample_results:\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Chunk #{result['chunk_index']} (similarity {result['similarity']:.3f})\")\n",
    "        print(result[\"text\"])\n",
    "else:\n",
    "    print(\"Vector store not found yet. Run the embedding cell above first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33cc785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "law-comparisons-9i-3jzyC-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
